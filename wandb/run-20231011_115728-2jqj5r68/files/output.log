MLPv3(
  (layer1): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=784, out_features=128, bias=True)
    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): LeakyReLU(negative_slope=0.01)
  )
  (layer2): Sequential(
    (0): Linear(in_features=128, out_features=64, bias=True)
    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01)
  )
  (layer3): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01)
  )
  (layer4): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01)
  )
  (layer5): Sequential(
    (0): Linear(in_features=16, out_features=10, bias=True)
  )
)
Loss after 03072 examples: 2.05488181
Loss after 06272 examples: 1.74347162
Loss after 09472 examples: 1.53202724
Loss after 12640 examples: 1.36497128
Loss after 15840 examples: 1.18826675
Loss after 19040 examples: 1.08527565
Loss after 22240 examples: 0.94780791
Loss after 25408 examples: 0.74059069
Loss after 28608 examples: 0.63130146
Loss after 31808 examples: 0.48566347
Loss after 35008 examples: 0.52271140
Loss after 38176 examples: 0.40965652
Loss after 41376 examples: 0.43927753
Loss after 44576 examples: 0.31321487
Loss after 47776 examples: 0.36119422
Loss after 50944 examples: 0.20962121
Loss after 54144 examples: 0.21854386
Loss after 57344 examples: 0.27258238
Loss after 60512 examples: 0.19338088
Loss after 63712 examples: 0.12676506
Loss after 66912 examples: 0.14703614
Loss after 70112 examples: 0.16040753
Loss after 73280 examples: 0.23176561
Loss after 76480 examples: 0.17041431
Loss after 79680 examples: 0.17240092
Loss after 82880 examples: 0.17845388
Loss after 86048 examples: 0.16210155
Loss after 89248 examples: 0.16176009
Loss after 92448 examples: 0.18611944
Loss after 95648 examples: 0.11040627
Loss after 98816 examples: 0.16988376
Loss after 102016 examples: 0.10357739
Loss after 105216 examples: 0.09607470
Loss after 108384 examples: 0.22563764
Loss after 111584 examples: 0.06590190
Loss after 114784 examples: 0.10524404
Loss after 117984 examples: 0.08797631
Loss after 121152 examples: 0.05876259
Loss after 124352 examples: 0.14953843
Loss after 127552 examples: 0.08373160
Loss after 130752 examples: 0.04847068
Loss after 133920 examples: 0.09202869
Loss after 137120 examples: 0.11104032
Loss after 140320 examples: 0.06844048
Loss after 143520 examples: 0.13072914
Loss after 146688 examples: 0.06348547
Loss after 149888 examples: 0.13929895
Loss after 153088 examples: 0.07186248
Loss after 156256 examples: 0.16927287
Loss after 159456 examples: 0.09966461
Loss after 162656 examples: 0.10649680
Loss after 165856 examples: 0.11255933
Loss after 169024 examples: 0.10334247
Loss after 172224 examples: 0.12889914
Loss after 175424 examples: 0.10307643
Loss after 178624 examples: 0.06138196
Loss after 181792 examples: 0.07714325
Loss after 184992 examples: 0.09531039
Loss after 188192 examples: 0.08146991
Loss after 191392 examples: 0.04790906
Loss after 194560 examples: 0.11299320
Loss after 197760 examples: 0.05846754
Loss after 200960 examples: 0.15097715
Loss after 204128 examples: 0.06185343
Loss after 207328 examples: 0.04554899
Loss after 210528 examples: 0.06030331
Loss after 213728 examples: 0.05669742
Loss after 216896 examples: 0.01197642
Loss after 220096 examples: 0.04669403
Loss after 223296 examples: 0.06562205
Loss after 226496 examples: 0.09891340
Loss after 229664 examples: 0.03671843
Loss after 232864 examples: 0.13577926
Loss after 236064 examples: 0.01874340
Loss after 239264 examples: 0.05214572
Accuracy of the model on the 2000 test images: 96.050000%
================ Diagnostic Run torch.onnx.export version 2.0.1 ================
verbose: False, log level: Level.ERROR
======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================