MLPv2(
  (layer1): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=784, out_features=64, bias=True)
    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU()
    (4): Dropout(p=0.5, inplace=False)
  )
  (layer2): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.5, inplace=False)
  )
  (layer3): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.5, inplace=False)
  )
  (layer4): Sequential(
    (0): Linear(in_features=16, out_features=10, bias=True)
  )
)
Loss after 03072 examples: 2.02525401
Loss after 06272 examples: 1.94393861
Loss after 09472 examples: 1.59736371
Loss after 12640 examples: 1.63445401
Loss after 15840 examples: 1.46980298
Loss after 19040 examples: 1.32883096
Loss after 22240 examples: 1.31987894
Loss after 25408 examples: 1.23412073
Loss after 28608 examples: 1.21759129
Loss after 31808 examples: 1.28784430
Loss after 35008 examples: 1.13655114
Loss after 38176 examples: 1.11375427
Loss after 41376 examples: 1.13932955
Loss after 44576 examples: 1.20664191
Loss after 47776 examples: 1.16503990
Loss after 50944 examples: 1.23555815
Loss after 54144 examples: 1.23481965
Loss after 57344 examples: 0.97071368
Loss after 60512 examples: 1.12687206
Loss after 63712 examples: 1.11731231
Loss after 66912 examples: 1.01850224
Loss after 70112 examples: 1.25558150
Loss after 73280 examples: 0.92987281
Loss after 76480 examples: 1.00747633
Loss after 79680 examples: 0.96321952
Loss after 82880 examples: 0.95860189
Loss after 86048 examples: 1.04092944
Loss after 89248 examples: 1.11273611
Loss after 92448 examples: 0.89250308
Loss after 95648 examples: 0.81412971
Loss after 98816 examples: 0.87570137
Loss after 102016 examples: 0.83111101
Loss after 105216 examples: 0.83717221
Loss after 108384 examples: 1.18162262
Loss after 111584 examples: 0.91449952
Loss after 114784 examples: 0.86192125
Loss after 117984 examples: 0.89624423
Loss after 121152 examples: 1.10169590
Loss after 124352 examples: 0.95239085
Loss after 127552 examples: 0.96454722
Loss after 130752 examples: 0.96454370
Loss after 133920 examples: 1.04252315
Loss after 137120 examples: 1.09084105
Loss after 140320 examples: 0.81783664
Loss after 143520 examples: 0.84451658
Loss after 146688 examples: 0.87160337
Loss after 149888 examples: 0.96879166
Loss after 153088 examples: 0.90471685
Loss after 156256 examples: 0.71789503
Loss after 159456 examples: 0.83851188
Loss after 162656 examples: 0.82561350
Loss after 165856 examples: 0.92280227
Loss after 169024 examples: 0.69955224
Loss after 172224 examples: 0.80551744
Loss after 175424 examples: 0.91069359
Loss after 178624 examples: 1.07986939
Loss after 181792 examples: 0.81416345
Loss after 184992 examples: 0.81877285
Loss after 188192 examples: 0.89808398
Loss after 191392 examples: 0.71958435
Loss after 194560 examples: 1.02535045
Loss after 197760 examples: 0.71567190
Loss after 200960 examples: 0.89496106
Loss after 204128 examples: 0.89093304
Loss after 207328 examples: 0.85290492
Loss after 210528 examples: 0.86225373
Loss after 213728 examples: 0.68478137
Loss after 216896 examples: 0.86631531
Loss after 220096 examples: 0.83142698
Loss after 223296 examples: 0.73736185
Loss after 226496 examples: 0.83883941
Loss after 229664 examples: 0.77690411
Loss after 232864 examples: 0.89249182
Loss after 236064 examples: 0.57174724
Loss after 239264 examples: 1.04257119
Accuracy of the model on the 2000 test images: 91.850000%
================ Diagnostic Run torch.onnx.export version 2.0.1 ================
verbose: False, log level: Level.ERROR
======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================