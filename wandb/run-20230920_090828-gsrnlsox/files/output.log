MLP(
  (layer1): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=784, out_features=64, bias=True)
    (2): Sigmoid()
  )
  (layer2): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): Sigmoid()
  )
  (layer3): Sequential(
    (0): Linear(in_features=32, out_features=16, bias=True)
    (1): Sigmoid()
  )
  (layer4): Sequential(
    (0): Linear(in_features=16, out_features=10, bias=True)
  )
)
Loss after 03072 examples: 2.27698183
Loss after 06272 examples: 2.15985703
Loss after 09472 examples: 1.88701868
Loss after 12640 examples: 1.64953816
Loss after 15840 examples: 1.45712543
Loss after 19040 examples: 1.33010602
Loss after 22240 examples: 1.22491336
Loss after 25408 examples: 1.14275312
Loss after 28608 examples: 1.08330584
Loss after 31808 examples: 0.94244623
Loss after 35008 examples: 1.00317729
Loss after 38176 examples: 0.89931905
Loss after 41376 examples: 0.88466334
Loss after 44576 examples: 0.84510064
Loss after 47776 examples: 0.86650550
Loss after 50944 examples: 0.69141620
Loss after 54144 examples: 0.55949420
Loss after 57344 examples: 0.63427198
Loss after 60512 examples: 0.52661538
Loss after 63712 examples: 0.66684616
Loss after 66912 examples: 0.51786417
Loss after 70112 examples: 0.48191783
Loss after 73280 examples: 0.48505864
Loss after 76480 examples: 0.45593286
Loss after 79680 examples: 0.40499541
Loss after 82880 examples: 0.37890425
Loss after 86048 examples: 0.33271632
Loss after 89248 examples: 0.34737062
Loss after 92448 examples: 0.31215668
Loss after 95648 examples: 0.30661407
Loss after 98816 examples: 0.30130574
Loss after 102016 examples: 0.22177544
Loss after 105216 examples: 0.23275137
Loss after 108384 examples: 0.23281755
Loss after 111584 examples: 0.26443481
Loss after 114784 examples: 0.22798786
Loss after 117984 examples: 0.20142254
Loss after 121152 examples: 0.13626471
Loss after 124352 examples: 0.16283676
Loss after 127552 examples: 0.25097978
Loss after 130752 examples: 0.13255379
Loss after 133920 examples: 0.11588106
Loss after 137120 examples: 0.13833536
Loss after 140320 examples: 0.23112455
Loss after 143520 examples: 0.12247367
Loss after 146688 examples: 0.10445692
Loss after 149888 examples: 0.07539038
Loss after 153088 examples: 0.14021963
Loss after 156256 examples: 0.14056703
Loss after 159456 examples: 0.10831369
Loss after 162656 examples: 0.05368836
Loss after 165856 examples: 0.06837580
Loss after 169024 examples: 0.08464152
Loss after 172224 examples: 0.04715000
Loss after 175424 examples: 0.03538412
Loss after 178624 examples: 0.05224720
Loss after 181792 examples: 0.08271858
Loss after 184992 examples: 0.10444126
Loss after 188192 examples: 0.04181004
Loss after 191392 examples: 0.06852659
Loss after 194560 examples: 0.04109856
Loss after 197760 examples: 0.04358175
Loss after 200960 examples: 0.08529226
Loss after 204128 examples: 0.05015112
Loss after 207328 examples: 0.09736393
Loss after 210528 examples: 0.03842804
Loss after 213728 examples: 0.03703840
Loss after 216896 examples: 0.02598962
Loss after 220096 examples: 0.02385028
Loss after 223296 examples: 0.02991324
Loss after 226496 examples: 0.04238524
Loss after 229664 examples: 0.01953876
Loss after 232864 examples: 0.01694477
Loss after 236064 examples: 0.02768249
Loss after 239264 examples: 0.03008951
Accuracy of the model on the 2000 test images: 94.200000%
================ Diagnostic Run torch.onnx.export version 2.0.1 ================
verbose: False, log level: Level.ERROR
======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================